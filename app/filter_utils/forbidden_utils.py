# app/filter_utils/forbidden_utils.py

import hgtk
import time
import ahocorasick
from app.database import get_connection
import app.state as state

# from konlpy.tag import Okt
# okt = Okt()

from konlpy.tag import Mecab
mecab = Mecab()
# mecab = Mecab(dicpath="/opt/homebrew/Cellar/mecab-ko-dic/2.1.1-20180720/lib/mecab/dic/mecab-ko-dic")

exclude_for_jamo = set()

def decompose_text(text: str) -> str:
    return hgtk.text.decompose(text).replace('á´¥', '')

def prepare_forbidden_entry(word: str) -> dict:
    decomposed = decompose_text(word)
    return {"word": word, "decomposed_word": decomposed}

def prepare_forbidden_entries(words: list[str]) -> list[dict]:
    return [{"word": word, "decomposed_word": decompose_text(word)} for word in words]

def load_automaton_from_db() -> ahocorasick.Automaton | None:
    """DBì—ì„œ ê¸ˆì¹™ì–´ ë¡œë”©í•˜ì—¬ íŠ¸ë¼ì´ ìƒì„±"""
    automaton = ahocorasick.Automaton()
    inserted_words = set()
    unique_original_words = set()   # ì‹¤ì œ ë“±ë¡ëœ ì›í˜• ë‹¨ì–´ë§Œ ë”°ë¡œ ì €ì¥

    conn = None

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT word, decomposed_word FROM forbidden_words")
        rows = cursor.fetchall()

        if not rows:
            print("âš ï¸ [ì£¼ì˜] ê¸ˆì¹™ì–´ê°€ DBì— ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None

        for word, decomposed in rows:
            if word and word not in inserted_words:
                automaton.add_word(word, (word, "original"))
                inserted_words.add(word)
                unique_original_words.add(word)

            if decomposed and word not in exclude_for_jamo:
                jamo = decomposed.replace(" ", "")
                if len(jamo) >= 3 and jamo not in inserted_words:
                    automaton.add_word(jamo, (word, "decomposed"))
                    inserted_words.add(jamo)

        automaton.make_automaton()
        state.forbidden_automaton = automaton  # âœ… ì „ì—­ ìƒíƒœì— ì§ì ‘ í• ë‹¹

        # print(f"âœ… ì´ {len(unique_original_words)}ê°œì˜ ê¸ˆì¹™ì–´ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"âœ… ì›í˜• {len(unique_original_words)}ê°œ, ìëª¨ {len(inserted_words - unique_original_words)}ê°œ ë“±ë¡ ì™„ë£Œ")
        return automaton

    except Exception as e:
        print(f"[âŒ ERROR] ê¸ˆì¹™ì–´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

    finally:
        if conn:
            conn.close()


def add_to_automaton(word: str, decomposed: str):
    """ê¸ˆì¹™ì–´ ë‹¨ì¼ ë“±ë¡ ì‹œ íŠ¸ë¼ì´ì— ë°˜ì˜"""
    if not state.forbidden_automaton:
        state.forbidden_automaton = ahocorasick.Automaton()

    # ì›í˜• ë‹¨ì–´ ë“±ë¡
    if word not in state.forbidden_automaton:
        state.forbidden_automaton.add_word(word, (word, "original"))
        print(f"âœ… [ì¶”ê°€] '{word}' â†’ ì›í˜• ê¸ˆì¹™ì–´ ë“±ë¡ ì™„ë£Œ")
    else:
        print(f"âš ï¸ [ì¤‘ë³µ] '{word}' â†’ ì´ë¯¸ ì›í˜• ë“±ë¡ë¨")

    # ìëª¨ ë‹¨ì–´ ë“±ë¡
    jamo = decomposed.replace(" ", "")
    if word not in exclude_for_jamo and len(jamo) >= 3:
        if jamo == word:
            print(f"âš ï¸ [ìƒëµ] '{jamo}' â†’ ì›í˜•ê³¼ ìëª¨ê°€ ê°™ìŒ (ë“±ë¡ ìƒëµ)")
        elif jamo not in state.forbidden_automaton:
            state.forbidden_automaton.add_word(jamo, (word, "decomposed"))
            print(f"âœ… [ì¶”ê°€] '{jamo}' â†’ ìëª¨ ê¸ˆì¹™ì–´ ë“±ë¡ ì™„ë£Œ")
        else:
            print(f"âš ï¸ [ì¤‘ë³µ] '{jamo}' â†’ ì´ë¯¸ ìëª¨ ë“±ë¡ë¨")

    # ê¼­ ë‹¤ì‹œ ë¹Œë“œí•´ì•¼ í•¨ (ahocorasickëŠ” build ì´í›„ì—ë§Œ íƒìƒ‰ ê°€ëŠ¥)
    state.forbidden_automaton.make_automaton()

ALLOWED_POS_PREFIXES = ('N', 'V', 'M', 'VA', 'XR', 'IC')  # ëª…ì‚¬, ë™ì‚¬, ë¶€ì‚¬, í˜•ìš©ì‚¬, ì–´ê·¼, ê°íƒ„ì‚¬

def extract_meaningful_tokens(message: str) -> list[str]:
    tokens = mecab.pos(message)
    return [token for token, pos in tokens if not pos.startswith('J') and pos.startswith(ALLOWED_POS_PREFIXES)]

def generate_ngrams(tokens: list[str], n_range=(2, 3)) -> set[str]:
    ngram_set = set()
    for n in range(n_range[0], n_range[1]+1):
        for i in range(len(tokens) - n + 1):
            ngram = ''.join(tokens[i:i+n])
            ngram_set.add(ngram)
    return ngram_set

def check_forbidden_message(message: str) -> dict:
    """ë©”ì‹œì§€ í•œ ê°œì— ëŒ€í•´ ê¸ˆì¹™ì–´ í¬í•¨ ì—¬ë¶€ ê²€ì‚¬ (í˜•íƒœì†Œ ê¸°ë°˜ ë‹¨ì–´ë§Œ ê²€ì‚¬)"""
    start_time = time.time()
    automaton = state.forbidden_automaton
    decomposed = decompose_text(message)

    tokens = extract_meaningful_tokens(message)
    token_set = set(tokens)
    ngram_set = generate_ngrams(tokens)
    meaningful_tokens = token_set | ngram_set  # ë‹¨ì–´ + ngram ë³‘í•©
    # print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼: {meaningful_tokens}")

    # 1ì°¨: ì›í˜• ê²€ì‚¬
    for _, (word, mode) in automaton.iter(message):
        if mode == "original" and word in meaningful_tokens:
            elapsed = time.time() - start_time
            return {
                "message": message,
                "result": "ğŸš« ê¸ˆì¹™ì–´ í¬í•¨",
                "detected_words": [word],
                "method": "ì›í˜•",
                "elapsed": round(elapsed, 4)
            }

    # 2ì°¨: ìëª¨ ê²€ì‚¬
    for _, (word, mode) in automaton.iter(decomposed):
        if mode == "decomposed" and word in meaningful_tokens:
            elapsed = time.time() - start_time
            return {
                "message": message,
                "result": "ğŸš« ê¸ˆì¹™ì–´ í¬í•¨",
                "detected_words": [word],
                "method": "ìëª¨",
                "elapsed": round(elapsed, 4)
            }

    # í†µê³¼
    elapsed = time.time() - start_time
    return {
        "message": message,
        "result": "âœ… í†µê³¼",
        "detected_words": [],
        "method": "-",
        "elapsed": round(elapsed, 4)
    }
    
    
def delete_forbidden_word(word: str) -> bool:
    conn = get_connection()
    cursor = conn.cursor()

    try:
        cursor.execute("DELETE FROM forbidden_words WHERE word = ?", (word,))
        conn.commit()
        return cursor.rowcount > 0  # ì‚­ì œëœ í–‰ì´ ìˆìœ¼ë©´ True

    except Exception as e:
        conn.rollback()
        print("âŒ ì‚­ì œ ì—ëŸ¬:", e)
        return False

    finally:
        conn.close()